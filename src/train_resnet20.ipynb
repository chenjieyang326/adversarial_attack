{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYBN0V-HwXx2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ru2BkXqwnEf"
      },
      "source": [
        "### 1. Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbP5m3hCwscb",
        "outputId": "be0f1452-11a5-4205-aee9-ec1f30b0b6f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Transformations for the input data with augmentation for the training set\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the images horizontally\n",
        "    transforms.RandomRotation(10),      # Randomly rotate the images by up to 10 degrees\n",
        "    transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),  # Random shear and scale\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR-10 normalization\n",
        "])\n",
        "\n",
        "# Use the same transformation as before for the test set (without augmentation)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Loading the CIFAR-10 datasets with transformations\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes in CIFAR-10 remain the same\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftr8UcWNw6GD"
      },
      "source": [
        "### 2. Define the ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmNQY4VvxBzC"
      },
      "outputs": [],
      "source": [
        "# Define the BasicBlock for ResNet\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.swish = Swish()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.swish(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.swish(out)\n",
        "        return out\n",
        "\n",
        "# Define the ResNet-20 model\n",
        "class ResNet20(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet20, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.swish = Swish()\n",
        "        self.layer1 = self.make_layer(16, 3, stride=1)\n",
        "        self.layer2 = self.make_layer(32, 3, stride=2)\n",
        "        self.layer3 = self.make_layer(64, 3, stride=2)\n",
        "        self.avg_pool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def make_layer(self, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.swish(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0xZkcW4xJ6J",
        "outputId": "0519182a-4620-40da-8ba9-a8c214ba2a4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet20(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (swish): Swish()\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (swish): Swish()\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (swish): Swish()\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (swish): Swish()\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (swish): Swish()\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (swish): Swish()\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (swish): Swish()\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (swish): Swish()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (swish): Swish()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (swish): Swish()\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
              "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = ResNet20()\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ggYCC7wxEGr"
      },
      "source": [
        "### 3. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1RkPwADxSXm"
      },
      "outputs": [],
      "source": [
        "# # momentum for optimizer\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "#############################################\n",
        "# create loss function\n",
        "criterion =  nn.CrossEntropyLoss()\n",
        "\n",
        "# Add optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=MOMENTUM,weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOv360X6xhRt",
        "outputId": "da1059fa-3ad2-4791-cf34-c8c7a34bcdc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0: Train Loss: 0.6704, Train Acc: 0.7688, Val Loss: 0.5683, Val Acc: 0.8125\n",
            "Epoch 1: Train Loss: 0.6345, Train Acc: 0.7803, Val Loss: 0.5882, Val Acc: 0.8059\n",
            "Epoch 2: Train Loss: 0.6138, Train Acc: 0.7873, Val Loss: 0.5646, Val Acc: 0.8094\n",
            "Epoch 3: Train Loss: 0.5765, Train Acc: 0.8021, Val Loss: 0.5321, Val Acc: 0.8226\n",
            "Epoch 4: Train Loss: 0.5525, Train Acc: 0.8094, Val Loss: 0.4939, Val Acc: 0.8342\n",
            "Epoch 5: Train Loss: 0.5326, Train Acc: 0.8153, Val Loss: 0.5139, Val Acc: 0.8237\n",
            "Epoch 6: Train Loss: 0.5210, Train Acc: 0.8186, Val Loss: 0.4644, Val Acc: 0.8424\n",
            "Epoch 7: Train Loss: 0.5031, Train Acc: 0.8253, Val Loss: 0.4711, Val Acc: 0.8412\n",
            "Epoch 8: Train Loss: 0.4877, Train Acc: 0.8301, Val Loss: 0.4446, Val Acc: 0.8533\n",
            "Epoch 9: Train Loss: 0.4754, Train Acc: 0.8349, Val Loss: 0.4707, Val Acc: 0.8425\n",
            "Epoch 10: Train Loss: 0.4639, Train Acc: 0.8406, Val Loss: 0.4311, Val Acc: 0.8573\n",
            "Epoch 11: Train Loss: 0.4520, Train Acc: 0.8420, Val Loss: 0.4546, Val Acc: 0.8478\n",
            "Epoch 12: Train Loss: 0.4408, Train Acc: 0.8464, Val Loss: 0.4390, Val Acc: 0.8510\n",
            "Epoch 13: Train Loss: 0.4295, Train Acc: 0.8501, Val Loss: 0.4056, Val Acc: 0.8636\n",
            "Epoch 14: Train Loss: 0.4211, Train Acc: 0.8523, Val Loss: 0.4151, Val Acc: 0.8551\n",
            "Epoch 15: Train Loss: 0.4118, Train Acc: 0.8564, Val Loss: 0.3997, Val Acc: 0.8649\n",
            "Epoch 16: Train Loss: 0.4061, Train Acc: 0.8601, Val Loss: 0.3962, Val Acc: 0.8653\n",
            "Epoch 17: Train Loss: 0.3961, Train Acc: 0.8630, Val Loss: 0.4121, Val Acc: 0.8622\n",
            "Epoch 18: Train Loss: 0.3921, Train Acc: 0.8632, Val Loss: 0.3910, Val Acc: 0.8685\n",
            "Epoch 19: Train Loss: 0.3845, Train Acc: 0.8658, Val Loss: 0.3934, Val Acc: 0.8657\n",
            "Epoch 20: Train Loss: 0.3820, Train Acc: 0.8676, Val Loss: 0.4063, Val Acc: 0.8623\n",
            "Epoch 21: Train Loss: 0.3704, Train Acc: 0.8706, Val Loss: 0.3901, Val Acc: 0.8695\n",
            "Epoch 22: Train Loss: 0.3618, Train Acc: 0.8742, Val Loss: 0.3834, Val Acc: 0.8723\n",
            "Epoch 23: Train Loss: 0.3641, Train Acc: 0.8749, Val Loss: 0.3800, Val Acc: 0.8722\n",
            "Epoch 24: Train Loss: 0.3566, Train Acc: 0.8753, Val Loss: 0.3720, Val Acc: 0.8742\n",
            "Epoch 25: Train Loss: 0.3528, Train Acc: 0.8759, Val Loss: 0.3689, Val Acc: 0.8752\n",
            "Epoch 26: Train Loss: 0.3463, Train Acc: 0.8774, Val Loss: 0.3672, Val Acc: 0.8759\n",
            "Epoch 27: Train Loss: 0.3387, Train Acc: 0.8822, Val Loss: 0.3741, Val Acc: 0.8757\n",
            "Epoch 28: Train Loss: 0.3362, Train Acc: 0.8830, Val Loss: 0.3751, Val Acc: 0.8745\n",
            "Epoch 29: Train Loss: 0.3398, Train Acc: 0.8820, Val Loss: 0.3551, Val Acc: 0.8811\n",
            "Epoch 30: Train Loss: 0.3268, Train Acc: 0.8851, Val Loss: 0.3674, Val Acc: 0.8762\n",
            "Epoch 31: Train Loss: 0.3263, Train Acc: 0.8857, Val Loss: 0.3517, Val Acc: 0.8825\n",
            "Epoch 32: Train Loss: 0.3188, Train Acc: 0.8883, Val Loss: 0.3551, Val Acc: 0.8819\n",
            "Epoch 33: Train Loss: 0.3194, Train Acc: 0.8893, Val Loss: 0.3519, Val Acc: 0.8806\n",
            "Epoch 34: Train Loss: 0.3186, Train Acc: 0.8870, Val Loss: 0.3699, Val Acc: 0.8786\n",
            "Epoch 35: Train Loss: 0.3200, Train Acc: 0.8879, Val Loss: 0.3592, Val Acc: 0.8795\n",
            "Epoch 36: Train Loss: 0.3084, Train Acc: 0.8928, Val Loss: 0.3466, Val Acc: 0.8856\n",
            "Epoch 37: Train Loss: 0.3069, Train Acc: 0.8921, Val Loss: 0.3486, Val Acc: 0.8841\n",
            "Epoch 38: Train Loss: 0.2978, Train Acc: 0.8956, Val Loss: 0.3496, Val Acc: 0.8857\n",
            "Epoch 39: Train Loss: 0.3002, Train Acc: 0.8937, Val Loss: 0.3513, Val Acc: 0.8826\n",
            "Epoch 40: Train Loss: 0.2977, Train Acc: 0.8945, Val Loss: 0.3509, Val Acc: 0.8839\n",
            "Epoch 41: Train Loss: 0.2934, Train Acc: 0.8972, Val Loss: 0.3485, Val Acc: 0.8856\n",
            "Epoch 42: Train Loss: 0.2887, Train Acc: 0.8980, Val Loss: 0.3556, Val Acc: 0.8815\n",
            "Epoch 43: Train Loss: 0.2862, Train Acc: 0.9001, Val Loss: 0.3645, Val Acc: 0.8828\n",
            "Epoch 44: Train Loss: 0.2840, Train Acc: 0.9002, Val Loss: 0.3503, Val Acc: 0.8849\n",
            "Epoch 45: Train Loss: 0.2835, Train Acc: 0.9002, Val Loss: 0.3511, Val Acc: 0.8835\n",
            "Epoch 46: Train Loss: 0.2870, Train Acc: 0.8982, Val Loss: 0.3517, Val Acc: 0.8829\n",
            "Epoch 47: Train Loss: 0.2765, Train Acc: 0.9033, Val Loss: 0.3414, Val Acc: 0.8881\n",
            "Epoch 48: Train Loss: 0.2768, Train Acc: 0.9033, Val Loss: 0.3546, Val Acc: 0.8865\n",
            "Epoch 49: Train Loss: 0.2774, Train Acc: 0.9036, Val Loss: 0.3410, Val Acc: 0.8875\n",
            "Epoch 50: Train Loss: 0.2765, Train Acc: 0.9032, Val Loss: 0.3434, Val Acc: 0.8880\n",
            "Epoch 51: Train Loss: 0.2739, Train Acc: 0.9040, Val Loss: 0.3478, Val Acc: 0.8881\n",
            "Epoch 52: Train Loss: 0.2756, Train Acc: 0.9028, Val Loss: 0.3429, Val Acc: 0.8878\n",
            "Epoch 53: Train Loss: 0.2724, Train Acc: 0.9035, Val Loss: 0.3414, Val Acc: 0.8881\n",
            "Epoch 54: Train Loss: 0.2681, Train Acc: 0.9067, Val Loss: 0.3511, Val Acc: 0.8862\n",
            "Epoch 55: Train Loss: 0.2666, Train Acc: 0.9066, Val Loss: 0.3543, Val Acc: 0.8872\n",
            "Epoch 56: Train Loss: 0.2668, Train Acc: 0.9061, Val Loss: 0.3435, Val Acc: 0.8870\n",
            "Epoch 57: Train Loss: 0.2702, Train Acc: 0.9037, Val Loss: 0.3409, Val Acc: 0.8903\n",
            "Epoch 58: Train Loss: 0.2619, Train Acc: 0.9074, Val Loss: 0.3401, Val Acc: 0.8873\n",
            "Epoch 59: Train Loss: 0.2654, Train Acc: 0.9061, Val Loss: 0.3530, Val Acc: 0.8858\n",
            "Epoch 60: Train Loss: 0.2610, Train Acc: 0.9077, Val Loss: 0.3372, Val Acc: 0.8913\n",
            "Epoch 61: Train Loss: 0.2648, Train Acc: 0.9067, Val Loss: 0.3336, Val Acc: 0.8904\n",
            "Epoch 62: Train Loss: 0.2591, Train Acc: 0.9084, Val Loss: 0.3330, Val Acc: 0.8916\n",
            "Epoch 63: Train Loss: 0.2600, Train Acc: 0.9082, Val Loss: 0.3434, Val Acc: 0.8874\n",
            "Epoch 64: Train Loss: 0.2574, Train Acc: 0.9108, Val Loss: 0.3378, Val Acc: 0.8892\n",
            "Epoch 65: Train Loss: 0.2573, Train Acc: 0.9090, Val Loss: 0.3435, Val Acc: 0.8873\n",
            "Epoch 66: Train Loss: 0.2557, Train Acc: 0.9105, Val Loss: 0.3426, Val Acc: 0.8890\n",
            "Epoch 67: Train Loss: 0.2554, Train Acc: 0.9114, Val Loss: 0.3349, Val Acc: 0.8907\n",
            "Epoch 68: Train Loss: 0.2535, Train Acc: 0.9105, Val Loss: 0.3390, Val Acc: 0.8872\n",
            "Epoch 69: Train Loss: 0.2528, Train Acc: 0.9104, Val Loss: 0.3377, Val Acc: 0.8912\n",
            "Epoch 70: Train Loss: 0.2526, Train Acc: 0.9114, Val Loss: 0.3442, Val Acc: 0.8887\n",
            "Epoch 71: Train Loss: 0.2552, Train Acc: 0.9109, Val Loss: 0.3377, Val Acc: 0.8902\n",
            "Epoch 72: Train Loss: 0.2508, Train Acc: 0.9110, Val Loss: 0.3470, Val Acc: 0.8863\n",
            "Epoch 73: Train Loss: 0.2544, Train Acc: 0.9110, Val Loss: 0.3365, Val Acc: 0.8924\n",
            "Epoch 74: Train Loss: 0.2496, Train Acc: 0.9120, Val Loss: 0.3344, Val Acc: 0.8926\n",
            "Epoch 75: Train Loss: 0.2527, Train Acc: 0.9111, Val Loss: 0.3435, Val Acc: 0.8892\n",
            "Epoch 76: Train Loss: 0.2510, Train Acc: 0.9121, Val Loss: 0.3305, Val Acc: 0.8948\n",
            "Epoch 77: Train Loss: 0.2495, Train Acc: 0.9108, Val Loss: 0.3367, Val Acc: 0.8910\n",
            "Epoch 78: Train Loss: 0.2501, Train Acc: 0.9128, Val Loss: 0.3404, Val Acc: 0.8926\n",
            "Epoch 79: Train Loss: 0.2489, Train Acc: 0.9135, Val Loss: 0.3419, Val Acc: 0.8920\n",
            "Epoch 80: Train Loss: 0.2469, Train Acc: 0.9135, Val Loss: 0.3348, Val Acc: 0.8914\n",
            "Epoch 81: Train Loss: 0.2478, Train Acc: 0.9125, Val Loss: 0.3407, Val Acc: 0.8916\n",
            "Epoch 82: Train Loss: 0.2478, Train Acc: 0.9134, Val Loss: 0.3336, Val Acc: 0.8915\n",
            "Epoch 83: Train Loss: 0.2482, Train Acc: 0.9128, Val Loss: 0.3342, Val Acc: 0.8921\n",
            "Epoch 84: Train Loss: 0.2486, Train Acc: 0.9111, Val Loss: 0.3374, Val Acc: 0.8904\n",
            "Epoch 85: Train Loss: 0.2464, Train Acc: 0.9139, Val Loss: 0.3320, Val Acc: 0.8925\n",
            "Epoch 86: Train Loss: 0.2437, Train Acc: 0.9146, Val Loss: 0.3400, Val Acc: 0.8908\n",
            "Epoch 87: Train Loss: 0.2485, Train Acc: 0.9131, Val Loss: 0.3332, Val Acc: 0.8933\n",
            "Epoch 88: Train Loss: 0.2460, Train Acc: 0.9129, Val Loss: 0.3383, Val Acc: 0.8899\n",
            "Epoch 89: Train Loss: 0.2417, Train Acc: 0.9149, Val Loss: 0.3382, Val Acc: 0.8903\n",
            "Epoch 90: Train Loss: 0.2460, Train Acc: 0.9126, Val Loss: 0.3338, Val Acc: 0.8924\n",
            "Epoch 91: Train Loss: 0.2439, Train Acc: 0.9140, Val Loss: 0.3399, Val Acc: 0.8904\n",
            "Epoch 92: Train Loss: 0.2457, Train Acc: 0.9141, Val Loss: 0.3371, Val Acc: 0.8910\n",
            "Epoch 93: Train Loss: 0.2479, Train Acc: 0.9133, Val Loss: 0.3303, Val Acc: 0.8931\n"
          ]
        }
      ],
      "source": [
        "# total number of training epochs\n",
        "EPOCHS = 200\n",
        "INITIAL_LR = 0.1\n",
        "grad_clip = 0.1\n",
        "best_val_acc = 0\n",
        "\n",
        "# Learning rate decay\n",
        "GAMMA = 0.95\n",
        "\n",
        "# Initialize lists for later visualization\n",
        "train_losses, train_accuracies = [], []\n",
        "val_losses, val_accuracies = [], []\n",
        "\n",
        "# Set the optimizer and the learning rate scheduler\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=INITIAL_LR)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=GAMMA)\n",
        "\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\" * 50)\n",
        "for epoch in range(EPOCHS):\n",
        "    net.train(True)\n",
        "\n",
        "    # Variables to track progress\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if grad_clip is not None:\n",
        "            nn.utils.clip_grad_value_(net.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_examples += targets.size(0)\n",
        "        correct_examples += (predicted == targets).sum().item()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Store training loss and accuracy\n",
        "    train_losses.append(train_loss / len(trainloader))\n",
        "    train_accuracies.append(correct_examples / total_examples)\n",
        "\n",
        "    # Validation loop\n",
        "    net.eval()\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_examples += targets.size(0)\n",
        "            correct_examples += (predicted == targets).sum().item()\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Store validation loss and accuracy\n",
        "    val_losses.append(val_loss / len(testloader))\n",
        "    val_accuracies.append(correct_examples / total_examples)\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save the model if better\n",
        "    if val_accuracies[-1] > best_val_acc:\n",
        "        best_val_acc = val_accuracies[-1]\n",
        "        torch.save(net.state_dict(), \"resnet_91_98.ckpt\")\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}